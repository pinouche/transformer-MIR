{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from numpy import exp, sqrt, dot\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.io import loadmat\n",
    "from random import shuffle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import moment\n",
    "from scipy.stats import skew\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"MODIS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    if dataset == \"CORN\" or dataset == \"WHEAT\":\n",
    "        full_data = pickle.load(open(dataset + \".p\", \"rb\"))\n",
    "        columns = ([\"id\"] +  ['label'] + [\"reflectance_\" + str(i) for i in range(92)])\n",
    "        full_data.columns = columns\n",
    "        \n",
    "    else:\n",
    "        mat_dict = loadmat('MIR/' + dataset + '.mat')\n",
    "        full_data = pd.DataFrame(mat_dict[dataset])\n",
    "\n",
    "        # Rename columns to something more interpretable\n",
    "        columns = ([\"id\"] + [\"reflectance_\" + str(i) for i in range(7)]\n",
    "                   + [\"solar_\" + str(i) for i in range(5)] + ['label'])\n",
    "        full_data.columns = columns\n",
    "\n",
    "        if dataset == \"MISR2\":\n",
    "            full_data = full_data[full_data['id'].isin(list(full_data['id'].value_counts().index[full_data['id'].value_counts() == 100]))]\n",
    "\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moments_df(n_moments):\n",
    "    \n",
    "    full_data = load_data()\n",
    "\n",
    "#     non_constant_cols = ['id', 'reflectance_0', 'reflectance_1', 'reflectance_2',\n",
    "#            'reflectance_3', 'reflectance_4', 'reflectance_5', 'reflectance_6']\n",
    "    \n",
    "    non_constant_cols = ([\"id\"] + [\"reflectance_\" + str(i) for i in range(12)])\n",
    "\n",
    "    constant_columns = ['solar_0', 'solar_1', 'solar_2', 'solar_3']\n",
    "    list_operation = ['np.mean', 'np.var', 'skew', 'kurtosis']\n",
    "\n",
    "    full_data_subset = full_data[non_constant_cols]\n",
    "    \n",
    "    dic_df = {}\n",
    "\n",
    "    for moments in range(1, n_moments+1):\n",
    "        if moments < 5:\n",
    "            dic_df[moments] = eval(\"np.stack(full_data_subset.groupby('id').apply(lambda group: \" + \n",
    "                                   list_operation[moments-1] + \"(group)).values)[:,1:]\")\n",
    "        else:\n",
    "            dic_df[moments] = np.stack(full_data_subset.groupby('id').apply(lambda group: moment(group, moment=moments)))[:,1:]\n",
    "\n",
    "    array_data = dic_df[1]\n",
    "    \n",
    "    if n_moments > 1:\n",
    "        for key in range(2, n_moments+1):\n",
    "            array_data = np.hstack([array_data, dic_df[key]])   \n",
    "\n",
    "    df = pd.DataFrame(array_data)\n",
    "    if dataset != \"CORN\" and dataset != \"WHEAT\":\n",
    "        df = pd.concat([df, full_data.groupby('id').mean()[constant_columns].reset_index()], axis=1)\n",
    "    df['id'] = full_data['id'].unique()\n",
    "    df['label'] = full_data.groupby('id').mean()['label'].values\n",
    "    df = df.set_index('id')\n",
    "    \n",
    "    col_label = ['label']\n",
    "    labels = df[col_label]\n",
    "    df = df[[col for col in df.columns if col not in col_label]]\n",
    "    \n",
    "    return df, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(train, test):\n",
    "    list_sequence_train, list_sequence_test = [], []\n",
    "    list_train_y, list_test_y = [], []\n",
    "    features = [col for col in train.columns if col not in ['id', 'label']]\n",
    "    \n",
    "    for bag_id in list(train['id'].unique()):\n",
    "        list_sequence_train.append(np.array(train[train['id']==bag_id][features]))\n",
    "        list_train_y.append(np.array(train[train['id']==bag_id]['label'])[0])\n",
    "        \n",
    "    for bag_id in list(test['id'].unique()):\n",
    "        list_sequence_test.append(np.array(test[test['id']==bag_id][features]))\n",
    "        list_test_y.append(np.array(test[test['id']==bag_id]['label'])[0])\n",
    "    \n",
    "    return list_sequence_train, list_sequence_test, list_train_y, list_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_attention(random_seed, splits=5, use_moments=False, n_moments=1):\n",
    "\n",
    "    full_data = load_data()\n",
    "    num_features = 12\n",
    "    count = 1\n",
    "    \n",
    "    if use_moments:\n",
    "        num_features = 92 + n_moments*92\n",
    "        df, _ = moments_df(n_moments)\n",
    "        col_solar = ['solar_0', 'solar_1', 'solar_2', 'solar_3', 'solar_4']\n",
    "        df = df.iloc[np.repeat(np.arange(len(df)), 100)]\n",
    "        df = df[[col for col in df.columns if col not in col_solar]]\n",
    "        df = df.reset_index(drop=True)\n",
    "        full_data = pd.concat([full_data, df], axis=1)\n",
    "    \n",
    "    kf = KFold(n_splits=splits, shuffle=True, random_state=random_seed)\n",
    "    cols_exclude = [\"id\", \"label\"]\n",
    "    features = [col for col in list(full_data.columns) if col not in cols_exclude]\n",
    "    lowest_val_list, loss_test_list = [], []\n",
    "    \n",
    "    for train_index, test_index in kf.split(list(full_data['id'].unique())):\n",
    "        #print('Compute for FOLD NUMBER ' + str(count))\n",
    "        train_index = full_data['id'].unique()[train_index]\n",
    "        test_index = full_data['id'].unique()[test_index]\n",
    "        train = full_data[full_data['id'].apply(lambda value: value in train_index)]\n",
    "        test = full_data[full_data['id'].apply(lambda value: value in test_index)]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train[features])\n",
    "        train[features], test[features] = scaler.transform(train[features]), scaler.transform(test[features])\n",
    "    \n",
    "        list_sequence_train, list_sequence_test, train_y, test_y = make_sequences(train, test)\n",
    "        lowest_val_loss, loss_test = transformer(list_sequence_train, list_sequence_test, train_y, test_y, \n",
    "                                               num_features)\n",
    "        \n",
    "        lowest_val_list.append(lowest_val_loss)\n",
    "        loss_test_list.append(loss_test)\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        count += 1\n",
    "    \n",
    "    return lowest_val_loss, loss_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(pred_array, truth_array):\n",
    "    loss = np.sqrt(mean_squared_error(np.reshape(np.array(pred_array), (np.array(pred_array).shape[0],1)), \n",
    "                                               np.reshape(np.array(truth_array), (np.array(truth_array).shape[0],1))))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the neighbours of the minimum to be sure it is not due to stochasticity\n",
    "def get_min_index(array, n_max=3):\n",
    "    indices = array.argsort()[:n_max] \n",
    "    \n",
    "    avg_neighbourgs = []\n",
    "    for val in indices:\n",
    "        if val == len(array)-1:\n",
    "            avg_neighbourgs.append((array[val]+array[val-1]+array[val-2])/3)\n",
    "        else:\n",
    "            avg_neighbourgs.append((array[val]+array[val-1]+array[val+1])/3)\n",
    "    \n",
    "    index = indices[np.argmin(avg_neighbourgs)]\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 12\n",
    "sequence_lenght = 100\n",
    "learning_rate = 0.00005\n",
    "n_epochs = 600\n",
    "num_test_runs = 1\n",
    "dropout_input = 1.0\n",
    "dropout_output = 0.7\n",
    "\n",
    "dim_transform1 = 16\n",
    "head_number = 1\n",
    "layer_number = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_with_moments(x, axes=0, epsilon=1e-8):\n",
    "    mean, variance = tf.nn.moments(x, axes=axes)\n",
    "    x_normed = (x - mean) / tf.sqrt(variance + epsilon) # epsilon to avoid dividing by zero\n",
    "    return x_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(q, k, v):\n",
    "    \n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True) \n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) \n",
    "\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_layer(X_or, layer_number, head_number, prob_output, multi_head_concat, num_features):\n",
    "    \n",
    "    with tf.variable_scope('attention_head_'+ str(layer_number) + '_' + str(head_number)):\n",
    "            \n",
    "            if layer_number > 1:\n",
    "                num_features = dim_transform1\n",
    "                \n",
    "            W_K = tf.Variable(tf.random_normal(shape = (num_features, dim_transform1), stddev = 0.1), name = \"weights_keys\", trainable = True)\n",
    "            W_Q = tf.Variable(tf.random_normal(shape = (num_features, dim_transform1), stddev = 0.1), name = \"weights_queries\", trainable = True)\n",
    "            W_V = tf.Variable(tf.random_normal(shape = (num_features, dim_transform1), stddev = 0.1), name = \"weights_values\", trainable = True)\n",
    "\n",
    "            queries = tf.matmul(X_or, W_K) # try with relu on these after that run finishes\n",
    "            keys = tf.matmul(X_or, W_Q)\n",
    "            values = tf.matmul(X_or, W_V)\n",
    "\n",
    "            output, attention_weights = self_attention(queries, keys, values)\n",
    "            output = tf.nn.dropout(output, prob_output, noise_shape=[1,dim_transform1])\n",
    "            #output = tf.add(values, output) # residual layer\n",
    "            #output = normalize_with_moments(output) # normalize residuals\n",
    "            multi_head_concat = tf.concat([multi_head_concat, output], axis = 1)\n",
    "    \n",
    "    return multi_head_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(list_sequence_train, list_sequence_val, train_y, val_y, num_features):\n",
    "    \n",
    "    with tf.variable_scope('data'):\n",
    "    \n",
    "        prob_input = tf.placeholder_with_default(1.0, shape=())\n",
    "        prob_output = tf.placeholder_with_default(1.0, shape=())\n",
    "        X = tf.placeholder(shape = [sequence_lenght, num_features], dtype = tf.float32, name = \"input\")\n",
    "        X_or = tf.nn.dropout(X, prob_input)\n",
    "        y = tf.placeholder(shape = [1,1], dtype = tf.float32, name = \"label\")\n",
    "        multi_head_concat = tf.zeros((sequence_lenght, dim_transform1), tf.float32)\n",
    "    \n",
    "    for layer in range(1, layer_number+1):\n",
    "        for head in range(1, head_number+1):\n",
    "\n",
    "            multi_head_concat = multi_head_layer(X_or, layer, head, prob_output, \n",
    "                                                 multi_head_concat = multi_head_concat, num_features=num_features)\n",
    "\n",
    "#         with tf.variable_scope('attention_head_'+ str(head)):\n",
    "\n",
    "#             W_K = tf.Variable(tf.random_normal(shape = (num_features, dim_transform1), stddev = 0.1), name = \"weights_keys\", trainable = True)\n",
    "#             W_Q = tf.Variable(tf.random_normal(shape = (num_features, dim_transform1), stddev = 0.1), name = \"weights_queries\", trainable = True)\n",
    "#             W_V = tf.Variable(tf.random_normal(shape = (num_features, dim_transform1), stddev = 0.1), name = \"weights_values\", trainable = True)\n",
    "\n",
    "#             queries = tf.matmul(X_or, W_K) # try with relu on these after that run finishes\n",
    "#             keys = tf.matmul(X_or, W_Q)\n",
    "#             values = tf.matmul(X_or, W_V)\n",
    "\n",
    "#             output, attention_weights = self_attention(queries, keys, values)\n",
    "#             output = tf.nn.dropout(output, prob_output, noise_shape=[1,dim_transform1])\n",
    "#             #output = tf.add(values, output) # residual layer\n",
    "#             #output = normalize_with_moments(output) # normalize residuals\n",
    "#             multi_head_concat = tf.concat([multi_head_concat, output], axis = 1)\n",
    "            \n",
    "        multi_head_concat = multi_head_concat[:,dim_transform1:]\n",
    "        dim_last = int(dim_transform1)\n",
    "        W_concat = tf.Variable(tf.random_normal(shape = (dim_transform1*head_number, dim_last), stddev = 0.1), name = \"weights_concat\", trainable = True)\n",
    "        X_or = tf.nn.relu(tf.matmul(multi_head_concat, W_concat))\n",
    "        multi_head_concat = tf.zeros((sequence_lenght, dim_last), tf.float32)\n",
    "    \n",
    "    with tf.variable_scope('logits'):\n",
    "    \n",
    "        #multi_head_concat = multi_head_concat[:,dim_transform1:]\n",
    "        # can do dropout here too\n",
    "        #projected_dim = dim\n",
    "        #W_concat = tf.Variable(tf.random_normal(shape = (dim_transform1*head_number, projected_dim), stddev = 0.1), name = \"weights_concat\", trainable = True)\n",
    "        #multi_head_concat = tf.nn.relu(tf.matmul(multi_head_concat, W_concat))\n",
    "\n",
    "        X_or = tf.reshape(tf.reshape(X_or, [-1]), shape = (sequence_lenght*dim_last,1)) #flatten the 2d array\n",
    "        X_or = tf.nn.dropout(X_or, prob_output, noise_shape=[sequence_lenght*dim_last,1])\n",
    "        W_final = tf.Variable(tf.random_normal(shape = (sequence_lenght*dim_last, 1), stddev = 0.1), name = \"weights_final\", trainable = True)\n",
    "        pred_label = tf.nn.relu(tf.matmul(tf.transpose(X_or), W_final))\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('loss'):\n",
    "    \n",
    "        loss =  tf.losses.mean_squared_error(predictions = pred_label, labels = y)\n",
    "        \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n",
    "    print(tf.trainable_variables())\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    val_loss_list, test_loss_list = [], []\n",
    "\n",
    "    for i in range(1, n_epochs+1):\n",
    "\n",
    "        train_pred, train_truth = [], []\n",
    "        val_test_pred, val_test_truth = [], []\n",
    "        indices = list(range(len(list_sequence_train)))\n",
    "        shuffle(indices)\n",
    "\n",
    "        for index in indices:\n",
    "            X_batch = np.reshape(list_sequence_train[index], (sequence_lenght, num_features))\n",
    "            Y_batch = np.reshape(train_y[index], (1,1))\n",
    "            _, pred = sess.run([train_op, pred_label], feed_dict = {X: X_batch, y: Y_batch, \n",
    "                                                                    prob_input: dropout_input, prob_output: dropout_output})\n",
    "\n",
    "            train_pred.append(pred)\n",
    "            train_truth.append(Y_batch)\n",
    "\n",
    "        for index in range(len(list_sequence_val)):\n",
    "            # batch is size 1 here: stochastic gradient descent\n",
    "            X_batch = np.reshape(list_sequence_val[index], (sequence_lenght, num_features))\n",
    "            Y_batch = np.reshape(val_y[index], (1,1))\n",
    "\n",
    "            pred_avg = 0\n",
    "            for _ in range(num_test_runs):\n",
    "                pred = sess.run([pred_label], feed_dict = {X: X_batch, y: Y_batch, prob_input: 1, prob_output: 1})\n",
    "                pred_avg += pred[0]\n",
    "\n",
    "            val_test_pred.append(pred_avg/num_test_runs)\n",
    "            val_test_truth.append(Y_batch)\n",
    "\n",
    "        if i == 1:\n",
    "            # shuffle because the bags in the list are ranked (increasing order)\n",
    "            range_arr = np.arange(len(val_test_truth))\n",
    "            half_lenght = int(len(val_test_truth)/2)\n",
    "            np.random.shuffle(range_arr)\n",
    "            indices_val, indices_test = range_arr[:half_lenght], range_arr[half_lenght:]\n",
    "\n",
    "        val_test_truth, val_test_pred = np.array(val_test_truth), np.array(val_test_pred)\n",
    "        val_pred, val_truth = val_test_pred[indices_val], val_test_truth[indices_val]\n",
    "        test_pred, test_truth = val_test_pred[indices_test], val_test_truth[indices_test]\n",
    "\n",
    "        training_loss = compute_rmse(train_pred, train_truth)\n",
    "        val_loss = compute_rmse(val_pred, val_truth)\n",
    "        test_loss = compute_rmse(test_pred, test_truth)\n",
    "\n",
    "        val_loss_list.append(val_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "\n",
    "        if (i == 1) or (i % 10 == 0):\n",
    "            print(\"EPOCH \" + str(i))\n",
    "            print(\"TRAINING LOSS: \" + str(training_loss))\n",
    "            print(\"VAL LOSS: \" + str(val_loss), \"TEST LOSS: \" + str(test_loss))\n",
    "\n",
    "    min_index = get_min_index(np.array(val_loss_list))\n",
    "    \n",
    "    lowest_val_loss = val_loss_list[min_index]\n",
    "    loss_test = test_loss_list[min_index]\n",
    "\n",
    "    return lowest_val_loss, loss_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOR DIM = 24\n",
      "[<tf.Variable 'attention_head_1_1/weights_keys:0' shape=(12, 16) dtype=float32_ref>, <tf.Variable 'attention_head_1_1/weights_queries:0' shape=(12, 16) dtype=float32_ref>, <tf.Variable 'attention_head_1_1/weights_values:0' shape=(12, 16) dtype=float32_ref>, <tf.Variable 'weights_concat:0' shape=(16, 16) dtype=float32_ref>, <tf.Variable 'attention_head_2_1/weights_keys:0' shape=(16, 16) dtype=float32_ref>, <tf.Variable 'attention_head_2_1/weights_queries:0' shape=(16, 16) dtype=float32_ref>, <tf.Variable 'attention_head_2_1/weights_values:0' shape=(16, 16) dtype=float32_ref>, <tf.Variable 'weights_concat_1:0' shape=(16, 16) dtype=float32_ref>, <tf.Variable 'attention_head_3_1/weights_keys:0' shape=(16, 16) dtype=float32_ref>, <tf.Variable 'attention_head_3_1/weights_queries:0' shape=(16, 16) dtype=float32_ref>, <tf.Variable 'attention_head_3_1/weights_values:0' shape=(16, 16) dtype=float32_ref>, <tf.Variable 'weights_concat_2:0' shape=(16, 16) dtype=float32_ref>, <tf.Variable 'logits/weights_final:0' shape=(1600, 1) dtype=float32_ref>]\n",
      "EPOCH 1\n",
      "TRAINING LOSS: 0.2450357314099478\n",
      "VAL LOSS: 0.201574036401229 TEST LOSS: 0.1486176194769971\n",
      "EPOCH 10\n",
      "TRAINING LOSS: 0.17056727244443698\n",
      "VAL LOSS: 0.17418794369995924 TEST LOSS: 0.10097281014470963\n",
      "EPOCH 20\n",
      "TRAINING LOSS: 0.16357696774140412\n",
      "VAL LOSS: 0.17531557715895218 TEST LOSS: 0.09799597519430323\n",
      "EPOCH 30\n",
      "TRAINING LOSS: 0.1476065126274838\n",
      "VAL LOSS: 0.17836785425341814 TEST LOSS: 0.10105288608389178\n",
      "EPOCH 40\n",
      "TRAINING LOSS: 0.14304942642598123\n",
      "VAL LOSS: 0.17733518621584687 TEST LOSS: 0.10068304128305619\n",
      "EPOCH 50\n",
      "TRAINING LOSS: 0.13863023512178227\n",
      "VAL LOSS: 0.17421329090357993 TEST LOSS: 0.10143129312450012\n",
      "EPOCH 60\n",
      "TRAINING LOSS: 0.14294275871807988\n",
      "VAL LOSS: 0.17250692642894425 TEST LOSS: 0.10734720145132613\n",
      "EPOCH 70\n",
      "TRAINING LOSS: 0.13254946029509454\n",
      "VAL LOSS: 0.16577339424394855 TEST LOSS: 0.10509968034251277\n",
      "EPOCH 80\n",
      "TRAINING LOSS: 0.13352948802318848\n",
      "VAL LOSS: 0.16415888556421832 TEST LOSS: 0.10727573934487193\n",
      "EPOCH 90\n",
      "TRAINING LOSS: 0.13871534718635647\n",
      "VAL LOSS: 0.16964751316911714 TEST LOSS: 0.1180417695484473\n",
      "EPOCH 100\n",
      "TRAINING LOSS: 0.12913700194946706\n",
      "VAL LOSS: 0.16783757156060772 TEST LOSS: 0.11711994095283983\n",
      "EPOCH 110\n",
      "TRAINING LOSS: 0.12485646473357365\n",
      "VAL LOSS: 0.16398005833461898 TEST LOSS: 0.11497913202138997\n",
      "EPOCH 120\n",
      "TRAINING LOSS: 0.11552160281933602\n",
      "VAL LOSS: 0.16695845962312936 TEST LOSS: 0.11691040671040415\n",
      "EPOCH 130\n",
      "TRAINING LOSS: 0.12057787661552502\n",
      "VAL LOSS: 0.16945691168853103 TEST LOSS: 0.12005094119015008\n",
      "EPOCH 140\n",
      "TRAINING LOSS: 0.12158409266914591\n",
      "VAL LOSS: 0.16719088618741976 TEST LOSS: 0.11898263978815683\n"
     ]
    }
   ],
   "source": [
    "for dim in [24]:\n",
    "    validation_loss_list, test_loss_list = [], []\n",
    "    print(\"FOR DIM = \" + str(dim))\n",
    "    lowest_val_loss, loss_test_list = inference_attention(1, 5, False, 1)\n",
    "    validation_loss_list.append(np.mean(lowest_val_loss))\n",
    "    test_loss_list.append(np.mean(loss_test_list))\n",
    "    print(np.mean(lowest_val_loss), np.mean(loss_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 128)\n",
      "2\n",
      "12\n",
      "128\n",
      "1536\n",
      "(12, 128)\n",
      "2\n",
      "12\n",
      "128\n",
      "1536\n",
      "(12, 128)\n",
      "2\n",
      "12\n",
      "128\n",
      "1536\n",
      "(128, 12)\n",
      "2\n",
      "128\n",
      "12\n",
      "1536\n",
      "(1200, 1)\n",
      "2\n",
      "1200\n",
      "1\n",
      "1200\n",
      "7344\n"
     ]
    }
   ],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    print(shape)\n",
    "    print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        print(dim)\n",
    "        variable_parameters *= dim.value\n",
    "    print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
