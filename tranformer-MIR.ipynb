{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from numpy import exp, sqrt, dot\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy.io import loadmat\n",
    "from random import shuffle\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "from scipy.stats import kurtosis\n",
    "from scipy.stats import moment\n",
    "from scipy.stats import skew\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"MODIS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \n",
    "    if dataset == \"CORN\" or dataset == \"WHEAT\":\n",
    "        full_data = pickle.load(open('MIR/' + dataset + \".p\", \"rb\"))\n",
    "        columns = ([\"id\"] +  ['label'] + [\"reflectance_\" + str(i) for i in range(92)])\n",
    "        full_data.columns = columns\n",
    "        \n",
    "    else:\n",
    "        mat_dict = loadmat('MIR/' + dataset + '.mat')\n",
    "        full_data = pd.DataFrame(mat_dict[dataset])\n",
    "\n",
    "        # Rename columns to something more interpretable\n",
    "        columns = ([\"id\"] + [\"reflectance_\" + str(i) for i in range(7)]\n",
    "                   + [\"solar_\" + str(i) for i in range(5)] + ['label'])\n",
    "        full_data.columns = columns\n",
    "\n",
    "        if dataset == \"MISR2\":\n",
    "            full_data = full_data[full_data['id'].isin(list(full_data['id'].value_counts().index[full_data['id'].value_counts() == 100]))]\n",
    "\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moments_df(n_moments):\n",
    "    \n",
    "    full_data = load_data()\n",
    "\n",
    "#     non_constant_cols = ['id', 'reflectance_0', 'reflectance_1', 'reflectance_2',\n",
    "#            'reflectance_3', 'reflectance_4', 'reflectance_5', 'reflectance_6']\n",
    "    \n",
    "    non_constant_cols = ([\"id\"] + [\"reflectance_\" + str(i) for i in range(12)])\n",
    "\n",
    "    constant_columns = ['solar_0', 'solar_1', 'solar_2', 'solar_3']\n",
    "    list_operation = ['np.mean', 'np.var', 'skew', 'kurtosis']\n",
    "\n",
    "    full_data_subset = full_data[non_constant_cols]\n",
    "    \n",
    "    dic_df = {}\n",
    "\n",
    "    for moments in range(1, n_moments+1):\n",
    "        if moments < 5:\n",
    "            dic_df[moments] = eval(\"np.stack(full_data_subset.groupby('id').apply(lambda group: \" + \n",
    "                                   list_operation[moments-1] + \"(group)).values)[:,1:]\")\n",
    "        else:\n",
    "            dic_df[moments] = np.stack(full_data_subset.groupby('id').apply(lambda group: moment(group, moment=moments)))[:,1:]\n",
    "\n",
    "    array_data = dic_df[1]\n",
    "    \n",
    "    if n_moments > 1:\n",
    "        for key in range(2, n_moments+1):\n",
    "            array_data = np.hstack([array_data, dic_df[key]])   \n",
    "\n",
    "    df = pd.DataFrame(array_data)\n",
    "    if dataset != \"CORN\" and dataset != \"WHEAT\":\n",
    "        df = pd.concat([df, full_data.groupby('id').mean()[constant_columns].reset_index()], axis=1)\n",
    "    df['id'] = full_data['id'].unique()\n",
    "    df['label'] = full_data.groupby('id').mean()['label'].values\n",
    "    df = df.set_index('id')\n",
    "    \n",
    "    col_label = ['label']\n",
    "    labels = df[col_label]\n",
    "    df = df[[col for col in df.columns if col not in col_label]]\n",
    "    \n",
    "    return df, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequences(train, test):\n",
    "    list_sequence_train, list_sequence_test = [], []\n",
    "    list_train_y, list_test_y = [], []\n",
    "    features = [col for col in train.columns if col not in ['id', 'label']]\n",
    "    \n",
    "    for bag_id in list(train['id'].unique()):\n",
    "        list_sequence_train.append(np.array(train[train['id']==bag_id][features]))\n",
    "        list_train_y.append(np.array(train[train['id']==bag_id]['label'])[0])\n",
    "        \n",
    "    for bag_id in list(test['id'].unique()):\n",
    "        list_sequence_test.append(np.array(test[test['id']==bag_id][features]))\n",
    "        list_test_y.append(np.array(test[test['id']==bag_id]['label'])[0])\n",
    "    \n",
    "    return list_sequence_train, list_sequence_test, list_train_y, list_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_attention(random_seed, splits=5, use_moments=False, n_moments=1):\n",
    "\n",
    "    full_data = load_data()\n",
    "    num_features = 12\n",
    "    count = 1\n",
    "    \n",
    "    if use_moments:\n",
    "        num_features = 92 + n_moments*92\n",
    "        df, _ = moments_df(n_moments)\n",
    "        col_solar = ['solar_0', 'solar_1', 'solar_2', 'solar_3', 'solar_4']\n",
    "        df = df.iloc[np.repeat(np.arange(len(df)), 100)]\n",
    "        df = df[[col for col in df.columns if col not in col_solar]]\n",
    "        df = df.reset_index(drop=True)\n",
    "        full_data = pd.concat([full_data, df], axis=1)\n",
    "    \n",
    "    kf = KFold(n_splits=splits, shuffle=True, random_state=random_seed)\n",
    "    cols_exclude = [\"id\", \"label\"]\n",
    "    features = [col for col in list(full_data.columns) if col not in cols_exclude]\n",
    "    lowest_val_list, loss_test_list = [], []\n",
    "    \n",
    "    for train_index, test_index in kf.split(list(full_data['id'].unique())):\n",
    "        #print('Compute for FOLD NUMBER ' + str(count))\n",
    "        train_index = full_data['id'].unique()[train_index]\n",
    "        test_index = full_data['id'].unique()[test_index]\n",
    "        train = full_data[full_data['id'].apply(lambda value: value in train_index)]\n",
    "        test = full_data[full_data['id'].apply(lambda value: value in test_index)]\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(train[features])\n",
    "        train[features], test[features] = scaler.transform(train[features]), scaler.transform(test[features])\n",
    "    \n",
    "        list_sequence_train, list_sequence_test, train_y, test_y = make_sequences(train, test)\n",
    "        lowest_val_loss, loss_test = transformer(list_sequence_train, list_sequence_test, train_y, test_y, \n",
    "                                               num_features)\n",
    "        \n",
    "        lowest_val_list.append(lowest_val_loss)\n",
    "        loss_test_list.append(loss_test)\n",
    "        \n",
    "        tf.reset_default_graph()\n",
    "        count += 1\n",
    "    \n",
    "    return lowest_val_loss, loss_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rmse(pred_array, truth_array):\n",
    "    loss = np.sqrt(mean_squared_error(np.reshape(np.array(pred_array), (np.array(pred_array).shape[0],1)), \n",
    "                                               np.reshape(np.array(truth_array), (np.array(truth_array).shape[0],1))))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the neighbours of the minimum to be sure it is not due to stochasticity\n",
    "def get_min_index(array, n_max=3):\n",
    "    indices = array.argsort()[:n_max] \n",
    "    \n",
    "    avg_neighbourgs = []\n",
    "    for val in indices:\n",
    "        if val == len(array)-1:\n",
    "            avg_neighbourgs.append((array[val]+array[val-1]+array[val-2])/3)\n",
    "        else:\n",
    "            avg_neighbourgs.append((array[val]+array[val-1]+array[val+1])/3)\n",
    "    \n",
    "    index = indices[np.argmin(avg_neighbourgs)]\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 12\n",
    "sequence_lenght = 100\n",
    "learning_rate = 0.0001\n",
    "n_epochs = 500\n",
    "num_test_runs = 1\n",
    "dropout_input = 1.0\n",
    "dropout_output = 0.5\n",
    "\n",
    "#dim_transform1 = 92\n",
    "head_number = 2\n",
    "layer_number = 1\n",
    "\n",
    "type_attention = \"hybrid\"\n",
    "lstm_size = 256\n",
    "processor_steps = 2\n",
    "\n",
    "if type_attention == \"self\" or type_attention == \"instance_self\":\n",
    "    dim_transform1 = 32\n",
    "    warmup_steps = 100\n",
    "    \n",
    "elif type_attention == \"hybrid\":\n",
    "    lstm_size = 256\n",
    "    dim_transform1 = int(lstm_size/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_with_moments(x, axes=0, epsilon=1e-8):\n",
    "    mean, variance = tf.nn.moments(x, axes=axes)\n",
    "    x_normed = (x - mean) / tf.sqrt(variance + epsilon) # epsilon to avoid dividing by zero\n",
    "    return x_normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(q, k, v):\n",
    "    \n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True) \n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1) \n",
    "\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "\n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_attention_lstm(X, lstm_size, dropout_holder, processor_steps=2):\n",
    "    \n",
    "    with tf.variable_scope('initialize_lstm'):\n",
    "        \n",
    "        static_input = tf.zeros(shape=[1,1,1])\n",
    "        unstack_input = tf.unstack(static_input, 1, 1)\n",
    "        lstm_cell = tf.nn.rnn_cell.LSTMCell(lstm_size, reuse=tf.AUTO_REUSE, state_is_tuple=True)\n",
    "        initial_state = lstm_cell.zero_state(1, dtype=tf.float32)\n",
    "        \n",
    "    for index in range(processor_steps):\n",
    "        with tf.variable_scope('lstm_cell_decoder', reuse = tf.AUTO_REUSE):\n",
    "            \n",
    "            output_decoder, state_decoder = tf.nn.static_rnn(lstm_cell, inputs=unstack_input,\n",
    "                                                             initial_state=initial_state, dtype=tf.float32)\n",
    "\n",
    "            W_decoder = tf.get_variable(initializer=tf.random_normal(shape = (lstm_size, int(lstm_size/2)), stddev = 0.1), name = \"decoder_weights\", trainable = True)\n",
    "\n",
    "            relu_decoder = tf.nn.relu(tf.matmul(state_decoder[-1], W_decoder))\n",
    "            relu_decoder = tf.nn.dropout(relu_decoder, dropout_holder, noise_shape=[1,int(lstm_size/2)])\n",
    "\n",
    "            inner_product = tf.matmul(X, tf.reshape(relu_decoder, shape=(int(lstm_size/2), 1)))\n",
    "            reshaped_inner_product = tf.reshape(inner_product, shape=(1, sequence_lenght))\n",
    "            \n",
    "            attention_score =  tf.nn.softmax(reshaped_inner_product)\n",
    "            context_vector = tf.multiply(X, tf.reshape(attention_score, (sequence_lenght, 1)))\n",
    "            r_vector = tf.reshape(tf.reduce_sum(context_vector, axis=0), (1,int(lstm_size/2)))\n",
    "            q_star = tf.concat([relu_decoder, r_vector], axis=1)\n",
    "\n",
    "            initial_state = (initial_state[0], q_star)\n",
    "            \n",
    "    with tf.variable_scope('logits'):\n",
    "        \n",
    "        W_logits = tf.Variable(tf.random_normal(shape = (lstm_size, 1), stddev = 0.1), name = \"weights_logits\", trainable = True)\n",
    "        logits = tf.nn.relu(tf.matmul(q_star, W_logits))\n",
    "        \n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_head_layer(X_or, layer_number, head_number, prob_output, multi_head_concat, num_features=num_features, dim_transform1=dim_transform1):\n",
    "    \n",
    "    with tf.variable_scope('attention_head_'+ str(layer_number) + '_' + str(head_number)):\n",
    "            \n",
    "            if layer_number > 1:\n",
    "                num_features = dim_transform1\n",
    "              \n",
    "            W_K = tf.Variable(tf.random_normal(shape = (num_features, dim_transform1), stddev = 0.1), name = \"weights_keys\", trainable = True)\n",
    "            W_Q = tf.Variable(tf.random_normal(shape = (num_features, dim_transform1), stddev = 0.1), name = \"weights_queries\", trainable = True)\n",
    "            W_V = tf.Variable(tf.random_normal(shape = (num_features, dim_transform1), stddev = 0.1), name = \"weights_values\", trainable = True)\n",
    "\n",
    "            queries = tf.matmul(X_or, W_K)\n",
    "            keys = tf.matmul(X_or, W_Q)\n",
    "            values = tf.matmul(X_or, W_V)\n",
    "\n",
    "            output, attention_weights = self_attention(queries, keys, values)\n",
    "            multi_head_concat = tf.concat([multi_head_concat, output], axis = 1)\n",
    "    \n",
    "    return multi_head_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer(list_sequence_train, list_sequence_val, train_y, val_y, num_features):\n",
    "    \n",
    "    with tf.variable_scope('data'):\n",
    "    \n",
    "        global_step = tf.Variable(1.0, name='global_step', trainable=False, dtype=tf.float32)\n",
    "        increment_op = tf.assign(global_step, global_step+1)\n",
    "        \n",
    "        prob_input = tf.placeholder_with_default(1.0, shape=())\n",
    "        prob_output = tf.placeholder_with_default(1.0, shape=())\n",
    "        X = tf.placeholder(shape = [sequence_lenght, num_features], dtype = tf.float32, name = \"input\")\n",
    "        X_or = tf.nn.dropout(X, prob_input, noise_shape=[1,num_features])\n",
    "        y = tf.placeholder(shape = [1,1], dtype = tf.float32, name = \"label\")\n",
    "        multi_head_concat = tf.zeros((sequence_lenght, dim_transform1), tf.float32)\n",
    "    \n",
    "    for layer in range(1, layer_number+1):\n",
    "        for head in range(1, head_number+1):\n",
    "\n",
    "            multi_head_concat = multi_head_layer(X_or, layer, head, prob_output, \n",
    "                                                 multi_head_concat = multi_head_concat, num_features=num_features)\n",
    "            \n",
    "        multi_head_concat = multi_head_concat[:,dim_transform1:]\n",
    "        multi_head_concat = tf.nn.dropout(multi_head_concat, prob_output, noise_shape=[1,dim_transform1*head_number])\n",
    "        W_concat = tf.Variable(tf.random_normal(shape = (dim_transform1*head_number, dim_transform1), stddev = 0.1), name = \"weights_concat\", trainable = True)\n",
    "        X_or = tf.nn.relu(tf.matmul(multi_head_concat, W_concat))\n",
    "        #X_or = tf.add(multi_head_concat, X_or) # residual \n",
    "        #X_or = normalize_with_moments(X_or) # nomralize\n",
    "        multi_head_concat = tf.zeros((sequence_lenght, dim_transform1), tf.float32)\n",
    "    \n",
    "    print(X_or) # this is dimension [sequence_lenght, projected_dimension]\n",
    "    if type_attention == \"self\":\n",
    "        print(\"TRANSFORMER\")\n",
    "        with tf.variable_scope('logits'):\n",
    "\n",
    "            X_or = tf.reshape(tf.reshape(X_or, [-1]), shape = (sequence_lenght*dim_transform1,1)) #flatten the 2d array\n",
    "            W_final = tf.Variable(tf.random_normal(shape = (sequence_lenght*dim_transform1, 1), stddev = 0.1), name = \"weights_final\", trainable = True)\n",
    "            pred_label = tf.nn.relu(tf.matmul(tf.transpose(X_or), W_final))\n",
    "            mean_pred = pred_label\n",
    "            \n",
    "    elif type_attention == \"instance_self\":\n",
    "        print(\"INSTANCE-TRANSFORMER\")\n",
    "        with tf.variable_scope('logits'):\n",
    "            \n",
    "            W_proj = tf.Variable(tf.random_normal(shape = (dim_transform1, dim_transform1*head_number), stddev = 0.1), name = \"weights_proj\", trainable = True)\n",
    "            h = tf.nn.relu(tf.matmul(X_or, W_proj))\n",
    "            #h = tf.nn.dropout(h, prob_output, noise_shape=[1,dim_transform1*head_number])\n",
    "            W_final = tf.Variable(tf.random_normal(shape = (dim_transform1*head_number, 1), stddev = 0.1), name = \"weights_final\", trainable = True)\n",
    "            pred_label = tf.nn.relu(tf.matmul(h, W_final))\n",
    "            mean_pred = tf.reduce_mean(pred_label, axis = 0)\n",
    "            y_array = tf.transpose(tf.tile(y, multiples= [1,sequence_lenght]))\n",
    "            \n",
    "    elif type_attention == \"hybrid\":\n",
    "        print(\"HYBRID\")\n",
    "        with tf.variable_scope('logits'):\n",
    "            pred_label = content_attention_lstm(X_or, lstm_size, prob_output)\n",
    "            mean_pred = pred_label\n",
    "        \n",
    "    with tf.variable_scope('loss'):\n",
    "        if type_attention == \"instance_self\":\n",
    "            loss =  tf.losses.mean_squared_error(predictions = pred_label, labels = y_array)\n",
    "        else:\n",
    "            loss =  tf.losses.mean_squared_error(predictions = pred_label, labels = y)\n",
    "            \n",
    "    \n",
    "    \n",
    "    warm_up_lr = tf.math.rsqrt(tf.constant(num_features,dtype=tf.float32)*tf.constant(head_number, dtype=tf.float32)*50) * tf.math.minimum(tf.math.rsqrt(global_step), global_step * (warmup_steps ** -1.5))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)\n",
    "    train_op = optimizer.minimize(loss)\n",
    "\n",
    "    print(tf.trainable_variables())\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess = tf.Session()\n",
    "    sess.run(init)\n",
    "\n",
    "    val_loss_list, test_loss_list = [], []\n",
    "\n",
    "    for i in range(1, n_epochs+1):\n",
    "        sess.run(increment_op)\n",
    "\n",
    "        train_pred, train_truth = [], []\n",
    "        val_test_pred, val_test_truth = [], []\n",
    "        indices = list(range(len(list_sequence_train)))\n",
    "        shuffle(indices)\n",
    "\n",
    "        for index in indices:\n",
    "            X_batch = np.reshape(list_sequence_train[index], (sequence_lenght, num_features))\n",
    "            Y_batch = np.reshape(train_y[index], (1,1))\n",
    "            _, pred = sess.run([train_op, mean_pred], feed_dict = {X: X_batch, y: Y_batch, \n",
    "                                                                    prob_input: dropout_input, prob_output: dropout_output})\n",
    "\n",
    "            train_pred.append(pred)\n",
    "            train_truth.append(Y_batch)\n",
    "\n",
    "        for index in range(len(list_sequence_val)):\n",
    "            # batch is size 1 here: stochastic gradient descent\n",
    "            X_batch = np.reshape(list_sequence_val[index], (sequence_lenght, num_features))\n",
    "            Y_batch = np.reshape(val_y[index], (1,1))\n",
    "\n",
    "            pred_avg = 0\n",
    "            for _ in range(num_test_runs):\n",
    "                pred = sess.run([mean_pred], feed_dict = {X: X_batch, y: Y_batch, prob_input: 1.0, prob_output: 1.0})\n",
    "                pred_avg += pred[0]\n",
    "\n",
    "            val_test_pred.append(pred_avg/num_test_runs)\n",
    "            val_test_truth.append(Y_batch)\n",
    "\n",
    "        if i == 1:\n",
    "            # shuffle because the bags in the list are ranked (increasing order)\n",
    "            range_arr = np.arange(len(val_test_truth))\n",
    "            half_lenght = int(len(val_test_truth)/2)\n",
    "            np.random.shuffle(range_arr)\n",
    "            indices_val, indices_test = range_arr[:half_lenght], range_arr[half_lenght:]\n",
    "\n",
    "        val_test_truth, val_test_pred = np.array(val_test_truth), np.array(val_test_pred)\n",
    "        val_pred, val_truth = val_test_pred[indices_val], val_test_truth[indices_val]\n",
    "        test_pred, test_truth = val_test_pred[indices_test], val_test_truth[indices_test]\n",
    "\n",
    "        training_loss = compute_rmse(train_pred, train_truth)\n",
    "        val_loss = compute_rmse(val_pred, val_truth)\n",
    "        test_loss = compute_rmse(test_pred, test_truth)\n",
    "\n",
    "        val_loss_list.append(val_loss)\n",
    "        test_loss_list.append(test_loss)\n",
    "\n",
    "        if (i == 1) or (i % 10 == 0):\n",
    "            print(\"EPOCH \" + str(i))\n",
    "            print(\"TRAINING LOSS: \" + str(training_loss))\n",
    "            print(\"VAL LOSS: \" + str(val_loss), \"TEST LOSS: \" + str(test_loss))\n",
    "            #print(sess.run(warm_up_lr))\n",
    "\n",
    "    min_index = get_min_index(np.array(val_loss_list))\n",
    "    \n",
    "    lowest_val_loss = val_loss_list[min_index]\n",
    "    loss_test = test_loss_list[min_index]\n",
    "\n",
    "    return lowest_val_loss, loss_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"Relu:0\", shape=(100, 128), dtype=float32)\n",
      "HYBRID\n",
      "[<tf.Variable 'attention_head_1_1/weights_keys:0' shape=(12, 128) dtype=float32_ref>, <tf.Variable 'attention_head_1_1/weights_queries:0' shape=(12, 128) dtype=float32_ref>, <tf.Variable 'attention_head_1_1/weights_values:0' shape=(12, 128) dtype=float32_ref>, <tf.Variable 'attention_head_1_2/weights_keys:0' shape=(12, 128) dtype=float32_ref>, <tf.Variable 'attention_head_1_2/weights_queries:0' shape=(12, 128) dtype=float32_ref>, <tf.Variable 'attention_head_1_2/weights_values:0' shape=(12, 128) dtype=float32_ref>, <tf.Variable 'weights_concat:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'logits/lstm_cell_decoder/rnn/lstm_cell/kernel:0' shape=(257, 1024) dtype=float32_ref>, <tf.Variable 'logits/lstm_cell_decoder/rnn/lstm_cell/bias:0' shape=(1024,) dtype=float32_ref>, <tf.Variable 'logits/lstm_cell_decoder/decoder_weights:0' shape=(256, 128) dtype=float32_ref>, <tf.Variable 'logits/logits/weights_logits:0' shape=(256, 1) dtype=float32_ref>]\n",
      "EPOCH 1\n",
      "TRAINING LOSS: 0.266010892771434\n",
      "VAL LOSS: 0.19693712155614512 TEST LOSS: 0.2118860032275386\n",
      "EPOCH 10\n",
      "TRAINING LOSS: 0.1692149993474595\n",
      "VAL LOSS: 0.1483354200059549 TEST LOSS: 0.1557320796903714\n",
      "EPOCH 20\n",
      "TRAINING LOSS: 0.14589983008073634\n",
      "VAL LOSS: 0.10729054920043676 TEST LOSS: 0.12566159963050094\n",
      "EPOCH 30\n",
      "TRAINING LOSS: 0.12457610111160981\n",
      "VAL LOSS: 0.1057352289492804 TEST LOSS: 0.12373341155186592\n",
      "EPOCH 40\n",
      "TRAINING LOSS: 0.12330137927590497\n",
      "VAL LOSS: 0.09345527528659445 TEST LOSS: 0.11316720390537224\n",
      "EPOCH 50\n",
      "TRAINING LOSS: 0.10520025842100009\n",
      "VAL LOSS: 0.09571608186191795 TEST LOSS: 0.10940252806104461\n",
      "EPOCH 60\n",
      "TRAINING LOSS: 0.10608315987281275\n",
      "VAL LOSS: 0.09373501844409207 TEST LOSS: 0.10238066498635283\n",
      "EPOCH 70\n",
      "TRAINING LOSS: 0.10278074375957864\n",
      "VAL LOSS: 0.09714352100532063 TEST LOSS: 0.10997782926042451\n",
      "EPOCH 80\n",
      "TRAINING LOSS: 0.09576365848793854\n",
      "VAL LOSS: 0.08266294078155063 TEST LOSS: 0.09882885921656812\n",
      "EPOCH 90\n",
      "TRAINING LOSS: 0.0926736858909468\n",
      "VAL LOSS: 0.09034338431999096 TEST LOSS: 0.09481684090434143\n",
      "EPOCH 100\n",
      "TRAINING LOSS: 0.09014508591381745\n",
      "VAL LOSS: 0.0814878794762921 TEST LOSS: 0.09909483982303348\n",
      "EPOCH 110\n",
      "TRAINING LOSS: 0.0856376537727232\n",
      "VAL LOSS: 0.08708068471937665 TEST LOSS: 0.10014310929808196\n"
     ]
    }
   ],
   "source": [
    "validation_loss_list, test_loss_list = [], []\n",
    "lowest_val_loss, loss_test_list = inference_attention(0, 5, False, 1)\n",
    "validation_loss_list.append(np.mean(lowest_val_loss))\n",
    "test_loss_list.append(np.mean(loss_test_list))\n",
    "print(np.mean(lowest_val_loss), np.mean(loss_test_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_parameters = 0\n",
    "for variable in tf.trainable_variables():\n",
    "    # shape is an array of tf.Dimension\n",
    "    shape = variable.get_shape()\n",
    "    print(shape)\n",
    "    print(len(shape))\n",
    "    variable_parameters = 1\n",
    "    for dim in shape:\n",
    "        print(dim)\n",
    "        variable_parameters *= dim.value\n",
    "    print(variable_parameters)\n",
    "    total_parameters += variable_parameters\n",
    "print(total_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
